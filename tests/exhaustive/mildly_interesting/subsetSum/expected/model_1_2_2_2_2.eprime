language ESSENCE' 1.0

given s: int
given fin1: int
given fin2: int
given fin3: int
given nums_Occurrence: matrix indexed by [int(fin2..fin3)] of bool
given nums_Explicit: matrix indexed by [int(1..fin1)] of int(fin2..fin3)
letting let1 be fin1
letting let2 be [i | i : int(fin2..fin3), nums_Occurrence[i]]
find x_ExplicitVarSizeWithFlags_Flags: matrix indexed by [int(1..let1)] of bool
find x_ExplicitVarSizeWithFlags_Values: matrix indexed by [int(1..let1)] of int(let2)
such that
    and([x_ExplicitVarSizeWithFlags_Flags[q9] ->
         or([nums_Explicit[q11] = x_ExplicitVarSizeWithFlags_Values[q9] | q11 : int(1..fin1)])
             | q9 : int(1..let1)]),
    s =
    sum([toInt(x_ExplicitVarSizeWithFlags_Flags[q7]) * catchUndef(x_ExplicitVarSizeWithFlags_Values[q7], 0)
             | q7 : int(1..let1)]),
    and([x_ExplicitVarSizeWithFlags_Flags[q2 + 1] ->
         x_ExplicitVarSizeWithFlags_Values[q2] < x_ExplicitVarSizeWithFlags_Values[q2 + 1]
             | q2 : int(1..let1 - 1)]),
    and([x_ExplicitVarSizeWithFlags_Flags[q3] = false -> x_ExplicitVarSizeWithFlags_Values[q3] = min(let2)
             | q3 : int(1..let1)]),
    and([x_ExplicitVarSizeWithFlags_Flags[q4 + 1] -> x_ExplicitVarSizeWithFlags_Flags[q4] | q4 : int(1..let1 - 1)]),
    1 <= sum([toInt(x_ExplicitVarSizeWithFlags_Flags[q5]) | q5 : int(1..let1)]),
    sum([toInt(x_ExplicitVarSizeWithFlags_Flags[q5]) | q5 : int(1..let1)]) <= let1

