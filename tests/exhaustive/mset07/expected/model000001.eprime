language ESSENCE' 1.0

given a: int
find x_ExplicitVarSizeWithFlags_Flags: matrix indexed by [int(1..2)] of int(0..min(2, 1 + (a - 1)))
find x_ExplicitVarSizeWithFlags_Values: matrix indexed by [int(1..2)] of int(1..a)
such that
    and([x_ExplicitVarSizeWithFlags_Flags[q1 + 1] > 0 -> x_ExplicitVarSizeWithFlags_Values[q1] < x_ExplicitVarSizeWithFlags_Values[q1 + 1] | q1 : int(1..2 - 1)]),
    and([x_ExplicitVarSizeWithFlags_Flags[q1] = 0 -> x_ExplicitVarSizeWithFlags_Values[q1] = 1 | q1 : int(1..2)]),
    and([x_ExplicitVarSizeWithFlags_Flags[q1 + 1] > 0 -> x_ExplicitVarSizeWithFlags_Flags[q1] > 0 | q1 : int(1..2 - 1)]),
    2 = sum([x_ExplicitVarSizeWithFlags_Flags[q1] | q1 : int(1..2)])

